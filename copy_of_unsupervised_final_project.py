# -*- coding: utf-8 -*-
"""Copy of Unsupervised Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QkT8y1fqbZ1rkx1M4IJVQJX-v-ubUUoZ
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import random
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls
import seaborn as sns
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib

from sklearn.mixture import GaussianMixture
from sklearn.metrics import mutual_info_score
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.decomposition import FastICA
from sklearn.manifold import MDS
from sklearn.manifold import TSNE
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
from sklearn.manifold import SpectralEmbedding


from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN

from scipy.stats import f_oneway
from scipy.stats import ttest_rel
from tqdm import tqdm

plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']

"""section 1:

  check how we access to the file by loacl path (or path to the drive)
  load data, remove the columns: dAge, dHispanic, iYearwrk and iSex, and id column

section 2:

  clustering algorithms: K-means, hierarchical, dbscan 
  determine num of clusters by k-means: check by elbow (x_axis is clusters num, y_axis is the loss of k-means)
  run 3 clusters algo with the num we determined before: calculate silluette and do ANOVA and T-test (for show the best algo)
  Figures: (1) elbow (2) silluette (bar plot) (3) silluette (by special graph) (4) 2-dim visualization of clustring data by the best algo

section 3:

  a. take the best cluster algo from previous section, and check which external feature has max MI with the clustered data 
  b. for each external (4 columns) we will evaluate by MI all clustering methods VS the external columns, 
    by the clusters number the each external feature has.
  Figures: (a) (1) bar plot: comparison between 4 external features (=x_axis), y_axis is the MI. (calculated by the best algo)
              (2) 2-dim visualization of clustering by best algo, show correlation with one external feature by color
          (b) bar plot: x_axis: 4 external features, y_axis is MI, purpose: show the best algo for each external (by the natural num of each external): 
              (do ANOVA, t-test separatly (4 - for each external))  + show results in table?
      


"""

from google.colab import drive
drive.mount('/content/drive')

####loading Data
all_data = pd.read_csv(open('drive/MyDrive/USCensus1990.data.csv', 'r'))
data = all_data.drop(columns = ['caseid', 'dAge', 'dHispanic', 'iYearwrk', 'iSex'])
N = len(data.index)
external_vars_list = ['dAge', 'dHispanic', 'iYearwrk', 'iSex']
external_variables = all_data[external_vars_list]

def kmeans(X, n):
    kmeans = KMeans(n_clusters=n)
    kmeans.fit(X)
    labels = kmeans.labels_
    return labels

def gmm(X, n):
    gmm = GaussianMixture(n_components=n)
    labels = gmm.fit_predict(X)
    return labels

def hierarchical(X, n):
    hc = AgglomerativeClustering(n_clusters=n)
    hc.fit(X)
    labels = hc.labels_
    return labels
    
def dbscan(X, n, eps = 5, prev = -1, step = 0.1, counter= 0):
    dbs = DBSCAN(eps=eps)
    dbs.fit(X)
    labels = list(dbs.labels_)
    max_labels = max(labels)
    if prev != -1 and abs(prev - max_labels) > 1 and (prev - n + 1)*(max_labels - n + 1) < 0:
      step = step/2
    if counter >= 15:
      return None, None, None
    if max(labels) > n - 1:
      print(f'labels : {max(labels) +1 }')
      return dbscan(X, n , eps=eps+step, prev = max(labels), step = step, counter=counter + 1)
    if max(labels) < n - 1:
      print(f'labels : {max(labels) +1 }')
      return dbscan(X, n_clusters, eps= eps-step, prev = max(labels), step = step, counter = counter + 1)
    print(f'correct labels : {max(labels) +1 }')
    dbscan_X = X.copy()
    idxs = []
    for idx, label in enumerate(labels):
        if label != -1:
            idxs.append(idx)
    dbscan_X =  dbscan_X.iloc[idxs]
    labels = [labels[i] for i in idxs]
    return dbscan_X, labels, idxs

def plot(X, labels, n_clusters, name=None):
    x_values = X[:,0]
    y_values = X[:,1]
    fig, ax = plt.subplots()
    for i in range(n_clusters):
        ix = np.where(labels == i)
        color = cm.nipy_spectral(float(i) / n_clusters)
        ax.scatter(x_values[ix], y_values[ix], c = color, label = i, s = 30)
    ax.legend()
    plt.xlabel('Dim 1')
    plt.ylabel('Dim 2')
    if name is not None:
      plt.savefig(name)
    plt.show()

# Kmeans Elbow Plot
losses = []
clusters = []
for n in range(3, 13):
  Kmeans = KMeans(n_clusters = n)
  Kmeans.fit(data)
  loss = Kmeans.inertia_ / N
  print(f'num_clusters: {n} loss: {loss}')
  losses.append(loss)
  clusters.append(n)
plt.plot(clusters, losses)
plt.xticks(clusters)
plt.xlabel('Cluster Number')
plt.ylabel('KMeans Loss')
plt.savefig('Fig1A_elbow_method.pdf')
plt.show()

#### Getting Silhouette scores for KMeans, Hirerchical Clustering, and DBscan
sample_size = 20000 
runs = 30
n_clusters = 5
kmeans_sils = []
hier_sils = []
dbscan_sils = []
gmm_sils = []

print('Running Clustering Algorithms')
for i in range(runs):
  print(f'Run {i+1} / {runs}')
  data_sample = data.sample(sample_size)
  kmeans_labels = kmeans(data_sample, n_clusters)
  kmeans_silhouette = silhouette_score(data_sample, kmeans_labels)
  hierarchical_labels = hierarchical(data_sample, n_clusters)
  hierarchical_silhouette = silhouette_score(data_sample, hierarchical_labels)
  dbscan_data, dbscan_labels, _ = dbscan(data_sample, n_clusters, eps=11.5)
  if dbscan_data is None:
    print('Cant find an epsilion that matches the required number of clusters')
    continue
  dbscan_silhouette = silhouette_score(dbscan_data, dbscan_labels)
  gmm_labels = gmm(data_sample, n_clusters)
  gmm_silhouette = silhouette_score(data_sample, gmm_labels)
  kmeans_sils.append(kmeans_silhouette)
  gmm_sils.append(gmm_silhouette)
  hier_sils.append(hierarchical_silhouette)
  dbscan_sils.append(dbscan_silhouette)
  
  print(f'Avg Silhouette Scores - Kmean: {kmeans_silhouette} Hierarchical: {hierarchical_silhouette} DBSCAN: {dbscan_silhouette} GMM: {gmm_silhouette}')

###performing Anova and paired T-test
print('ANOVA: ', f_oneway(kmeans_sils, hier_sils, dbscan_sils, gmm_sils))
print('T-test (K-means, DBSCAN):', ttest_rel(kmeans_sils, dbscan_sils))

###Plotting bar plot
dbscan_sils = np.array(dbscan_sils)
kmeans_sils = np.array(kmeans_sils)
hier_sils = np.array(hier_sils)
gmm_sils = np.array(gmm_sils)

dbscan_mean = np.mean(dbscan_sils)
kmeans_mean = np.mean(kmeans_sils)
hier_mean = np.mean(hier_sils)
gmm_mean = np.mean(gmm_sils)

dbscan_std = np.std(dbscan_sils)
kmeans_std = np.std(kmeans_sils)
hier_std = np.std(hier_sils)
gmm_std = np.std(gmm_sils)

algos = ['DBSCAN', 'K-Means', 'Hierarchical', 'GMM']
means = [dbscan_mean, kmeans_mean, hier_mean, gmm_mean]
stds = [dbscan_std, kmeans_std, hier_std, gmm_std]
x_pos = np.arange(len(algos))

print(f'Means- Kmeans: {kmeans_mean} Hierarchical: {hier_mean} DBSCAN: {dbscan_mean} GMM: {gmm_mean}')
print(f'Stds - Kmeans: {kmeans_std}, Hierarchical: {hier_std} DBSCAN: {dbscan_std} GMM: {gmm_std}')

fig, ax = plt.subplots()
ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, ecolor='black', capsize=10)
ax.set_ylabel('Silhouette Score')
ax.set_xticks(x_pos)
ax.set_xticklabels(algos)
ax.yaxis.grid(True)

# Save the figure and show
plt.tight_layout()
plt.savefig('Fig1B_n=5.pdf')
plt.show()

###Plotting Silhouette of DBSCAN
n_clusters = 5
sample_size = 1000

data_sample = data.sample(sample_size)
dbscan_data, dbscan_labels, idxs = dbscan(data_sample, n_clusters, eps=11.2)

silhouette_avg = silhouette_score(dbscan_data, dbscan_labels)
print(f'Avg Silhouette Score: {silhouette_avg}')
sample_silhouette_values = silhouette_samples(dbscan_data, dbscan_labels)
fig, ax = plt.subplots()
y_lower = 10
for i in range(n_clusters):
  # Aggregate the silhouette scores for samples belonging to
  # cluster i, and sort them
  ith_cluster_silhouette_values = sample_silhouette_values[np.array(dbscan_labels) == i]
  ith_cluster_silhouette_values.sort()

  size_cluster_i = ith_cluster_silhouette_values.shape[0]
  y_upper = y_lower + size_cluster_i

  color = cm.nipy_spectral(float(i) / n_clusters)
  ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)
  ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
  y_lower = y_upper + 10  # 10 for the 0 samples
  ax.set_xlabel("Silhouette Score")
  ax.set_ylabel("Cluster Label")
  ax.set_yticks([])  # Clear the yaxis labels / ticks
  ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
plt.savefig('Silhouette_figure.pdf')
plt.show()



###plotting with TSNE
tsne = TSNE(n_components=2)
new_X = tsne.fit_transform(dbscan_data)
plot(new_X, np.array(dbscan_labels), n_clusters)

#### Getting mutual information on different external variables on the best clustring acording to the last section

n_clusters = 5
sample_size = 20000
MI = {}
for var in external_vars_list:
  MI[var] = []

for i in range(30):
  data_sample = data.sample(sample_size)
  dbscan_data, dbscan_labels, idxs = dbscan(data_sample, n_clusters, eps=11.2)
  if dbscan_data is None:
    continue
  external_sample = external_variables.iloc[dbscan_data.index]
  for var in external_vars_list:
    mutual_info = mutual_info_score(external_sample[var].to_numpy(), dbscan_labels)
    MI[var].append(mutual_info)
    print(f'External Variable {var} Mutual Information {mutual_info}')


###performing Anova and paired T-test
print('ANOVA: ', f_oneway(MI['dAge'], MI['dHispanic'], MI['iYearwrk'], MI['iSex']))
print('T-test (dAge, iYearwrk):', ttest_rel(MI['dAge'], MI['iYearwrk']))


means = []
stds = []
for var in external_vars_list:
  means.append(np.mean(MI[var]))
  stds.append(np.std(MI[var]))

x_pos = np.arange(len(external_vars_list))

fig, ax = plt.subplots()
ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, ecolor='black', capsize=10)
ax.set_ylabel('MI')
ax.set_xticks(x_pos)
ax.set_xticklabels(external_vars_list)
ax.yaxis.grid(True)

# Save the figure and show
plt.tight_layout()
plt.savefig('MI_per_external.pdf')
plt.show()

### Getting MI for each clustring algorithm 


## Num of clusters per external variable
num_clusters_var = {}
for var in external_vars_list:
  num_clusters_var[var] = external_variables[var].max() + 1

num_clusters_var = {}
MI = {}
epsilons = {'dAge': 11.5, 'dHispanic': 11, 'iYearwrk': 11.5, 'iSex': 100}
for var in external_vars_list:
  num_clusters_var[var] = external_variables[var].max() + 1
  MI[var] = {'kmeans': [], 'hier': [], 'dbscan': [], 'gmm': []}

for var in external_vars_list:
  print(f'External Variable {var}')
  n_clusters = num_clusters_var[var]
  sample_size = 20000

  for i in range(30):
    print(f'Run {i+1}/30')
    data_sample = data.sample(sample_size)
    dbscan_data, dbscan_labels, _ = dbscan(data_sample, n_clusters, eps=epsilons[var])
    if dbscan_data is None:
      print('Cant find an epsilion that matches the required number of clusters')
      continue
    external_sample_dbscan = external_variables.iloc[dbscan_data.index]
    external_sample = external_variables.iloc[data_sample.index]
    kmeans_labels = kmeans(data_sample, n_clusters)
    kmeans_mi = mutual_info_score(external_sample[var].to_numpy(), kmeans_labels)
    hierarchical_labels = hierarchical(data_sample, n_clusters)
    hierarchical_mi = mutual_info_score(external_sample[var].to_numpy(), hierarchical_labels)

    dbscan_mi = mutual_info_score(external_sample_dbscan[var].to_numpy(), dbscan_labels)
    gmm_labels = gmm(data_sample, n_clusters)
    gmm_mi = mutual_info_score(external_sample[var].to_numpy(), gmm_labels)
    MI[var]['kmeans'].append(kmeans_mi)
    MI[var]['hier'].append(hierarchical_mi)
    MI[var]['dbscan'].append(dbscan_mi)
    MI[var]['gmm'].append(gmm_mi)
    print(f'MI- kmeans: {kmeans_mi} hier: {hierarchical_mi} dbscan: {dbscan_mi} gmm: {gmm_mi}')
  print('ANOVA: ', f_oneway(MI[var]['kmeans'], MI[var]['hier'], MI[var]['dbscan'], MI[var]['gmm']))
  print('T-test (GMM, Kmeans):', ttest_rel(MI[var]['kmeans'], MI[var]['gmm']))

plot_MI_bar(MI)

num_clusters_var = {}
MI = {}
epsilons = {'dAge': 11.5, 'dHispanic': 11, 'iYearwrk': 11.5, 'iSex': 100}
for var in external_vars_list:
  num_clusters_var[var] = external_variables[var].max() + 1
  MI[var] = {'kmeans': [], 'hier': [], 'dbscan': [], 'gmm': []}

for var in external_vars_list:
  print(f'External Variable {var}')
  n_clusters = num_clusters_var[var]
  sample_size = 20000

  for i in range(30):
    print(f'Run {i+1}/30')
    data_sample = data.sample(sample_size)
    dbscan_data, dbscan_labels, _ = dbscan(data_sample, n_clusters, eps=epsilons[var])
    if dbscan_data is None:
      print('Cant find an epsilion that matches the required number of clusters')
      continue
    external_sample_dbscan = external_variables.iloc[dbscan_data.index]
    external_sample = external_variables.iloc[data_sample.index]
    kmeans_labels = kmeans(data_sample, n_clusters)
    kmeans_mi = mutual_info_score(external_sample[var].to_numpy(), kmeans_labels)
    hierarchical_labels = hierarchical(data_sample, n_clusters)
    hierarchical_mi = mutual_info_score(external_sample[var].to_numpy(), hierarchical_labels)

    dbscan_mi = mutual_info_score(external_sample_dbscan[var].to_numpy(), dbscan_labels)
    gmm_labels = gmm(data_sample, n_clusters)
    gmm_mi = mutual_info_score(external_sample[var].to_numpy(), gmm_labels)
    MI[var]['kmeans'].append(kmeans_mi)
    MI[var]['hier'].append(hierarchical_mi)
    MI[var]['dbscan'].append(dbscan_mi)
    MI[var]['gmm'].append(gmm_mi)
    print(f'MI- kmeans: {kmeans_mi} hier: {hierarchical_mi} dbscan: {dbscan_mi} gmm: {gmm_mi}')
  print('ANOVA: ', f_oneway(MI[var]['kmeans'], MI[var]['hier'], MI[var]['dbscan'], MI[var]['gmm']))
  print('T-test (GMM, Kmeans):', ttest_rel(MI[var]['kmeans'], MI[var]['gmm']))

plot_MI_bar(MI)

def plot_MI_bar(my_dict):
    ticks = my_dict.keys()
    fig, ax = plt.subplots()
    # set width of bar
    barWidth = 0.2

    for ind, (variable_name, value) in enumerate(my_dict.items()):
        kmeans_MI = value['kmeans']
        hier_MI = value['hier']
        dbscan_MI = value['dbscan']
        gmm_MI = value['gmm']

        kmeans_mean = np.mean(kmeans_MI)
        hier_mean = np.mean(hier_MI)
        dbscan_mean = np.mean(dbscan_MI)
        gmm_mean = np.mean(gmm_MI)

        # Calculate the standard deviation
        kmeans_std = np.std(kmeans_MI)
        hier_std = np.std(hier_MI)
        dbscan_std = np.std(dbscan_MI)
        gmm_std = np.std(gmm_MI)


        # Build the plot

        # Set position of bar on X axis
        br1 = np.arange(1)
        br1[0] = ind
        br2 = [x + barWidth for x in br1]
        br3 = [x + barWidth for x in br2]
        br4 = [x + barWidth for x in br3]

        # Make the plot
        kmean = plt.bar(br1, kmeans_mean, width=barWidth, color='m', label='KMeans')
        hier = plt.bar(br2, hier_mean, width=barWidth, color='turquoise', label='hierarchical')
        dbscan = plt.bar(br3, dbscan_mean, width=barWidth, color='limegreen',  label='DBSCAN')
        gmm = plt.bar(br4, gmm_mean, width=barWidth, color='red', label='GMM')

        plt.errorbar(br1, kmeans_mean, yerr=kmeans_std, capsize=10,ecolor='k', elinewidth=3)
        plt.errorbar(br2, hier_mean, yerr=hier_std, capsize=10, ecolor='k',elinewidth=3)
        plt.errorbar(br3, dbscan_mean, yerr=dbscan_std, capsize=10,ecolor='k', elinewidth=3)
        plt.errorbar(br4, gmm_mean, yerr=gmm_std, capsize=10,ecolor='k', elinewidth=3)

    # Adding Xticks
    plt.ylabel('MI')
    plt.grid(axis = 'y')
    plt.xticks([r + 1.5*barWidth for r in range(len(ticks))],
               ticks)

    # ax.set_ylabel('Mutual Information')
    plt.tight_layout()
    # ax.legend()
    # plt.legend(["kmean", "AC", "DBSCAN"])
    ax.legend(handles=[kmean, hier, dbscan, gmm])
    plt.savefig('multi_bar_MI.pdf')
    plt.show()

n_clusters = 8
sample_size = 1000

var = 'iYearwrk'
data_sample = data.sample(sample_size)
#dbscan_data, dbscan_labels, idxs = dbscan(data_sample, n_clusters, eps=11.2)
labels = gmm(data_sample, n_clusters)
external_sample = external_variables.iloc[data_sample.index]

pca = PCA(n_components=2)
pca.fit(data_sample)
pca_X = pca.transform(data_sample)

silhouette_pca = silhouette_score(pca_X, np.array(external_sample[var]))
print(f'pca_silhouette_avg {silhouette_pca}')
plot(pca_X, np.array(external_sample[var]), n_clusters)

mds = MDS(n_components=2)
mds_X = mds.fit_transform(data_sample)

silhouette_mds = silhouette_score(mds_X, np.array(external_sample[var]))
print(f'mds_silhouette_avg {silhouette_mds}')
plot(mds_X, np.array(external_sample[var]), n_clusters, name = f'mds_labels_clusters_{var}.pdf')
plot(mds_X, np.array(labels), n_clusters, name = f'mds_gmm_clusters_{var}.pdf')

isomap = Isomap(n_components=2, n_neighbors=7)
isomap_X = isomap.fit_transform(data_sample)

silhouette_isomap = silhouette_score(isomap_X, np.array(external_sample[var]))
print(f'isomap_silhouette_avg {silhouette_isomap}')
plot(isomap_X, np.array(external_sample[var]), n_clusters)

lle = LocallyLinearEmbedding(n_components=2, n_neighbors=7)
lle_X = lle.fit_transform(data_sample)

silhouette_lle = silhouette_score(lle_X, np.array(external_sample[var]))
print(f'lle_silhouette_avg {silhouette_lle}')
plot(lle_X, np.array(external_sample[var]), n_clusters)

lem = SpectralEmbedding(n_components=2, n_neighbors=8)
lem_X = lem.fit_transform(data_sample)

silhouette_lem = silhouette_score(lem_X, np.array(external_sample[var]))
print(f'lem_silhouette_avg {silhouette_lem}')
plot(lem_X, np.array(external_sample[var]), n_clusters)

ica = FastICA(n_components=2)
ica_X = ica.fit_transform(data_sample)

silhouette_ica = silhouette_score(ica_X, np.array(external_sample[var]))
print(f'lem_silhouette_avg {silhouette_ica}')
plot(ica_X, np.array(external_sample[var]), n_clusters)

tsne = TSNE(n_components=2)
tsne_X = tsne.fit_transform(data_sample)

silhouette_tsne = silhouette_score(tsne_X, np.array(external_sample[var]))
print(f'tsne_silhouette_avg {silhouette_tsne}')
plot(tsne_X, np.array(external_sample[var]), n_clusters)
plot(tsne_X, np.array(labels), n_clusters )

print(f'Silhouettes: PCA {silhouette_pca} MDS {silhouette_mds} Isomap {silhouette_isomap} LLE {silhouette_lle} LEM {silhouette_lem} TSNE {silhouette_tsne}')

def dim_reduction_comaprison(extr):
  num_clusters_var = {}
  for var in external_vars_list:
    num_clusters_var[var] = external_variables[var].max() + 1
  n_clusters = num_clusters_var[extr]
  sample_size = 1000

  pca_sils = []
  mds_sils = []
  isomap_sils = []
  lle_sils = []
  lem_sils = []
  ica_sils = []
  tsne_sils = []

  for i in range(30):
    print(f'Run {i+1}/30')
    dbscan_data = data.sample(sample_size)
    #dbscan_data, dbscan_labels, idxs = dbscan(data_sample, n_clusters, eps=11.2)
    if dbscan_data is None:
      continue
    external_sample = external_variables.iloc[dbscan_data.index]

    pca = PCA(n_components=2)
    pca.fit(dbscan_data)
    pca_X = pca.transform(dbscan_data)

    silhouette_pca = silhouette_score(pca_X, np.array(external_sample[extr]))
    pca_sils.append(silhouette_pca)

    mds = MDS(n_components=2)
    mds_X = mds.fit_transform(dbscan_data)

    silhouette_mds = silhouette_score(mds_X, np.array(external_sample[extr]))
    mds_sils.append(silhouette_mds)

    isomap = Isomap(n_components=2, n_neighbors=20)
    isomap_X = isomap.fit_transform(dbscan_data)

    silhouette_isomap = silhouette_score(isomap_X, np.array(external_sample[extr]))
    isomap_sils.append(silhouette_isomap)

    lle = LocallyLinearEmbedding(n_components=2, n_neighbors=20)
    lle_X = lle.fit_transform(dbscan_data)

    silhouette_lle = silhouette_score(lle_X, np.array(external_sample[extr]))
    lle_sils.append(silhouette_lle)

    lem = SpectralEmbedding(n_components=2, n_neighbors=8)
    lem_X = lem.fit_transform(dbscan_data)

    silhouette_lem = silhouette_score(lem_X, np.array(external_sample[extr]))
    lem_sils.append(silhouette_lem)

    ica = FastICA(n_components=2)
    ica_X = ica.fit_transform(dbscan_data)

    silhouette_ica = silhouette_score(ica_X, np.array(external_sample[extr]))
    ica_sils.append(silhouette_ica)

    tsne = TSNE(n_components=2)
    tsne_X = tsne.fit_transform(dbscan_data)

    silhouette_tsne = silhouette_score(tsne_X, np.array(external_sample[extr]))
    tsne_sils.append(silhouette_tsne)
    print(f'Silhouettes: PCA {silhouette_pca} MDS {silhouette_mds} Isomap {silhouette_isomap} LLE {silhouette_lle} LEM {silhouette_lem} ICA {silhouette_ica} TSNE {silhouette_tsne}')

  ###performing Anova and paired T-test
  print('ANOVA: ', f_oneway(pca_sils, mds_sils, isomap_sils, lle_sils, lem_sils, tsne_sils))
  print('T-test (MDS, TSNE):', ttest_rel(mds_sils, tsne_sils))

  ###Plotting bar plot
  pca_sils = np.array(pca_sils)
  mds_sils = np.array(mds_sils)
  isomap_sils = np.array(isomap_sils)
  lle_sils = np.array(lle_sils)
  lem_sils = np.array(lem_sils)
  ica_sils = np.array(ica_sils)
  tsne_sils = np.array(tsne_sils)

  pca_mean = np.mean(pca_sils)
  mds_mean = np.mean(mds_sils)
  isomap_mean = np.mean(isomap_sils)
  lle_mean = np.mean(lle_sils)
  lem_mean = np.mean(lem_sils)
  ica_mean = np.mean(ica_sils)
  tsne_mean = np.mean(tsne_sils)

  pca_std = np.std(pca_sils)
  mds_std = np.std(mds_sils)
  isomap_std = np.std(isomap_sils)
  lle_std = np.std(lle_sils)
  lem_std = np.std(lem_sils)
  ica_std = np.std(ica_sils)
  tsne_std = np.std(tsne_sils)

  algos = ['PCA', 'MDS', 'Isomap', 'LLE', 'LEM', 'ICA', 'TSNE']
  means = [pca_mean, mds_mean, isomap_mean, lle_mean, lem_mean, ica_mean, tsne_mean]
  stds = [pca_std, mds_std, isomap_std, lle_std, lem_std, ica_std, tsne_std]
  print(f'Means for {algos} : {means}')
  x_pos = np.arange(len(algos))


  fig, ax = plt.subplots()
  ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, ecolor='black', capsize=10)
  ax.set_ylabel('Supervised Silhouette Score')
  ax.set_xticks(x_pos)
  ax.set_xticklabels(algos)
  ax.yaxis.grid(True)

  # Save the figure and show
  plt.tight_layout()
  plt.savefig(f'dim_reduction_silhouettes_{extr}.pdf')
  plt.show()

dim_reduction_comaprison('dAge')